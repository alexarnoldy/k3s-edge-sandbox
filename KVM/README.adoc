## Overview:

This is a Terraform powered K3s edge sandbox project that deploys multiple K3s clusters in a single KVM host and registers them to a Rancher server. The goal is to simulate multiple edge environments. 

IMPORTANT: This doesn't seem to work on hosts that currently have virtual bridges configured. It works fine on a KVM host with no VMs or virtual networks configured.

The number of clusters plus the number and roles of the cluster nodes is managed through the KVM host's /etc/hosts file (see examples below). 

Deploying this project will (hopefully) require little previous Terraform experience. 

It's based on openSUSE 15.2, which is compatible with SLES 15 SP2, so this could potentially be used for prototyping edge deployments for production workloads.

## Architectural notes:

* Each deployed K3s cluster uses a dedicated NAT bridge, and thus is not easily reachable outside of the local KVM host
* Currently, this edge sandbox environment must be installed on and run from on a single KVM host
** Certain solutions, such as SSH tunnels, need to be tested that would allow this environment to be run from a location other than the target KVM host
* If needed, the Rancher MCM server can be run as a Docker container in the local environment
* All of the simulated edge locations are defined through the /etc/hosts file 
** See the examples below for specific guidance

## Instructions for implementing: 

.The following required software can obtained from various sources but all are available from software.opensuse.org
* kubectl
** https://software.opensuse.org/package/kubectl
* terraform-provider-libvirt (also installs Terraform)
** https://software.opensuse.org/package/terraform-provider-libvirt
* k3sup
** https://software.opensuse.org/package/k3sup
* openSUSE-Leap-15.2-JeOS.x86_64-15.2-OpenStack-Cloud-Build31.348.qcow2
** https://get.opensuse.org/leap/
*** Select JeOS, then OpenStack-Cloud

=== Steps to prepare this environment

NOTE: It is recommended to verify the KVM host can deploy a VM and a NAT network before performing the steps described below.

* Deploy a Rancher MCM server anywhere that it can be reached by the K3s clusters on port 443
** Public cloud providers are common option as they allow easy access from the Internet or private networks with outward HTTPS access
** Various deployment options can be found at: https://rancher.com/docs/rancher/v2.x/en/quick-start-guide/deployment/
* Create API access tokens for your account on the Rancher MCM server 
** https://rancher.com/docs/rancher/v2.x/en/user-settings/api-keys/
* On the KVM host, create a ~/.rancher_tokens file with the format:

----
export RANCHER_URL=https://xxxx.xxxx.xxxx
export RANCHER_ACCESS_KEY=token-xxxxx
export RANCHER_SECRET_KEY=xxxxxxxxxxxxxxxx
----

* On the KVM host, clone this repository and cd into the directory
* Place the openSUSE-Leap-15.2-JeOS.x86_64-15.2-OpenStack-Cloud.qcow2 image in the images/ directory
* Update the terraform.tfvars file with: 
** The SSH public key for the KVM host user (the authorized_keys list at the bottom of the file)
* Update the rancher2.tf file with:
** Uncomment "insecure = true" if using self-signed certificates and not providing them to Terraform
* Update the KVM host's /etc/host file to define the simulated edge locations, their nodes, the vcpu/memory allocation per node, and the labels (on the server line only) to be applied to the clusters
** See the examples below for specific guidance
** Cluster labels are used by Rancher server for several things, including Continous Delivery
* From inside this directory, run `terraform init`
* Create a simulated edge location with command `./bin/k3s-cluster-create.sh <edge location> <optional domain name>`
** I.e. `./bin/k3s-cluster-create.sh cancun edge.sandbox.local`


NOTE: All servers must have "server" in their name and agents must have "agent" in their name. All nodes for a specific edge location must have the name of that edge location either in their name or domain name.

NOTE: server_vcpu and server_memory should be specified on the first server entry only (based on IP address). As well, agent_vcpu and agent_memory should be specified on the first agent entry only.

## Example of /etc/hosts entries:

----
#### K3ai Sandbox Demo with just hostnames
10.111.1.11	bangkok-server-0 #server_vcpu=2 server_memory=4096 labels: "location" = "north", "customer" = "BigMoney", "status" = "standby"
10.111.1.14	bangkok-agent-0 #agent_vcpu=4 agent_memory=8192
10.111.1.15	bangkok-agent-1

#### K3ai Sandbox Demo with a single node, acts as both server and agent
10.111.2.11	freetown-server-0 #server_vcpu=1 server_memory=2048 labels: "status" = "standby"

#### K3ai Sandbox Demo with a multiple server nodes (Requires load balancer that serves all server nodes for port 6443)
10.111.3.11	server-0-munich #server_vcpu=2 server_memory=2048 labels: "status" = "maintenance"
10.111.3.12	server-1-munich
10.111.3.13	server-2-munich
10.111.3.14	agent-0-munich #agent_vcpu=4 agent_memory=8192
10.111.3.15	agent-1-munich
10.111.3.16	agent-2-munich

#### K3ai Sandbox Demo with a two nodes
10.111.4.11	server.sydney.sandbox.local #server_vcpu=1 server_memory=1024 labels: "department" = "marketing"
10.111.4.14	agent.sydney.sandbox.local #agent_vcpu=4 agent_memory=8192
 
#### K3ai Sandbox Demo with FQDNs, note that order in which the nodes are listed in not important
10.111.5.13	cancun-server-2.edge.sandbox.local
10.111.5.12	cancun-server-1.edge.sandbox.local
10.111.5.11	cancun-server-0.edge.sandbox.local #server_vcpu=2 server_memory=2048 labels: "application" = "ml"
10.111.5.15	cancun-agent-1.edge.sandbox.local
10.111.5.14	cancun-agent-0.edge.sandbox.local #agent_vcpu=4 agent_memory=4096
10.111.5.16	cancun-agent-2.edge.sandbox.local
----
